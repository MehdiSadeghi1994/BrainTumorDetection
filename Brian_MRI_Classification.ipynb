{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.8"},"colab":{"name":"Brian_MRI_Classification.ipynb","provenance":[{"file_id":"https://github.com/pytorch/tutorials/blob/gh-pages/_downloads/62840b1eece760d5e42593187847261f/transfer_learning_tutorial.ipynb","timestamp":1598165871597}],"collapsed_sections":[]},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"c0f1a4a7212744baab0ccfb3842ea1c9":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_9734328003f946029f18655055b389eb","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_15f0649e3ce2481796e452b1f06d337e","IPY_MODEL_15e593a16d72486fbe41f1ebb392362a"]}},"9734328003f946029f18655055b389eb":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"15f0649e3ce2481796e452b1f06d337e":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_view_name":"ProgressView","style":"IPY_MODEL_35348b3d6b2149a8ae79680d88659c68","_dom_classes":[],"description":"100%","_model_name":"FloatProgressModel","bar_style":"success","max":46827520,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":46827520,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_c2029bb8fccb401b99346e5eb3c6a5da"}},"15e593a16d72486fbe41f1ebb392362a":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_9a234e7d86084763bc89a56ec40d61b5","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 44.7M/44.7M [00:00&lt;00:00, 175MB/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_02146dd15cb84ad5ae9c346ee3b0f911"}},"35348b3d6b2149a8ae79680d88659c68":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"c2029bb8fccb401b99346e5eb3c6a5da":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"9a234e7d86084763bc89a56ec40d61b5":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"02146dd15cb84ad5ae9c346ee3b0f911":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}}}}},"cells":[{"cell_type":"code","metadata":{"id":"L_tkJYOeG_8Z","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1631352118929,"user_tz":-270,"elapsed":10910,"user":{"displayName":"Mehdi Sadeghi","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"06637468399330131472"}},"outputId":"99a0258c-b846-4c07-ef4b-c2af57898ef5"},"source":["!pip install SimpleITK"],"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting SimpleITK\n","  Downloading SimpleITK-2.1.1-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (48.4 MB)\n","\u001b[K     |████████████████████████████████| 48.4 MB 5.0 kB/s \n","\u001b[?25hInstalling collected packages: SimpleITK\n","Successfully installed SimpleITK-2.1.1\n"]}]},{"cell_type":"code","metadata":{"id":"46mEY2eV1pi5","executionInfo":{"status":"ok","timestamp":1631352122557,"user_tz":-270,"elapsed":1614,"user":{"displayName":"Mehdi Sadeghi","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"06637468399330131472"}}},"source":["\n","#import packages\n","from __future__ import print_function, division\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torch.optim import lr_scheduler\n","import numpy as np\n","import torchvision\n","from torchvision import datasets, models, transforms\n","import matplotlib.pyplot as plt\n","import time\n","import os\n","import copy\n","import shutil\n","import SimpleITK as sitk\n","import cv2 as cv\n","from sklearn.metrics import accuracy_score, confusion_matrix\n","import itertools\n","\n","plt.ion()  "],"execution_count":3,"outputs":[]},{"cell_type":"code","metadata":{"id":"Iq1BvQ2IpGCM"},"source":["#connect to google drive\n","from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"lAH9vAryVsEC"},"source":["# Preparing Data"]},{"cell_type":"code","metadata":{"id":"H2uWxSEOVqrl","executionInfo":{"status":"ok","timestamp":1631352159242,"user_tz":-270,"elapsed":302,"user":{"displayName":"Mehdi Sadeghi","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"06637468399330131472"}}},"source":["#bias_correction: get the MR image and return the bias corrected image as a result\n","def bias_correction(image):\n","    corrector = sitk.N4BiasFieldCorrectionImageFilter(); #define a N4 bias corrector\n","    input_img = sitk.GetImageFromArray(np.copy(image)) #convert the numpy arry to \n","    maskImage = sitk.OtsuThreshold( input_img, 0, 1, 200 ) #Apply Otsu thresholding inorder to get the ROI of image\n","    input_img = sitk.Cast( input_img, sitk.sitkFloat32 )#Cast image to 'stikfloat32' dtype\n","    output = corrector.Execute( input_img, maskImage )   #Execute N4 method on image\n","    corrected_image = sitk.GetArrayFromImage(output)\n","    corrected_image = (corrected_image-corrected_image.min())/(corrected_image.max()-corrected_image.min()) #Normalizing the result\n","    return corrected_image\n","\n","#reshape_image: get the image and resize it to the target size \n","def reshape_image(image, size=(299, 299)):\n","    reshaped_image = cv.resize(image, size, cv.INTER_CUBIC)  #resize input image to 'size' using cubic interpolation  \n","    return reshaped_image\n","\n","#crop_image: crop the ROI region and return it\n","def crop_image(image):\n","    gray = cv.cvtColor(image, cv.COLOR_RGB2GRAY)#convert a RGB image to gray level image\n","    gray = cv.GaussianBlur(gray, (5, 5), 0) #Apply the gaussian filter inordet to remove the noise\n","\n","\n","    thresh = cv.threshold(gray, 45, 255, cv.THRESH_BINARY)[1]  #Apply thresholding to get the ROI region\n","    #the two next line delet the noise from the binary image using the erode and dilate\n","    thresh = cv.erode(thresh, None, iterations=2)\n","    thresh = cv.dilate(thresh, None, iterations=2)\n","\n","    # find contours in thresholded image, then grab the largest one\n","    cnts = cv.findContours(thresh.copy(), cv.RETR_EXTERNAL, cv.CHAIN_APPROX_SIMPLE) \n","    cnts = imutils.grab_contours(cnts)\n","    c = max(cnts, key=cv.contourArea)\n","    \n","    # find the extreme points\n","    extLeft = tuple(c[c[:, :, 0].argmin()][0])\n","    extRight = tuple(c[c[:, :, 0].argmax()][0])\n","    extTop = tuple(c[c[:, :, 1].argmin()][0])\n","    extBot = tuple(c[c[:, :, 1].argmax()][0])\n","\n","    # add contour on the image\n","    img_cnt = cv.drawContours(image.copy(), [c], -1, (0, 255, 255), 4)\n","\n","    # add extreme points\n","    img_pnt = cv.circle(img_cnt.copy(), extLeft, 8, (0, 0, 255), -1)\n","    img_pnt = cv.circle(img_pnt, extRight, 8, (0, 255, 0), -1)\n","    img_pnt = cv.circle(img_pnt, extTop, 8, (255, 0, 0), -1)\n","    img_pnt = cv.circle(img_pnt, extBot, 8, (255, 255, 0), -1)\n","\n","    # crop\n","    ADD_PIXELS = 0\n","    new_img = image[extTop[1]-ADD_PIXELS:extBot[1]+ADD_PIXELS, extLeft[0]-ADD_PIXELS:extRight[0]+ADD_PIXELS].copy()\n","    return new_img\n","    \n","\n","def plot_confusion_matrix(cm, classes,\n","                          normalize=False,\n","                          title='Confusion matrix',\n","                          cmap=plt.cm.Blues):\n","\n","    plt.figure(figsize = (6,6))\n","    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n","    plt.title(title)\n","    plt.colorbar()\n","    tick_marks = np.arange(len(classes))\n","    plt.xticks(tick_marks, classes, rotation=90)\n","    plt.yticks(tick_marks, classes)\n","    if normalize:\n","        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n","\n","    thresh = cm.max() / 2.\n","    cm = np.round(cm,2)\n","    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n","        plt.text(j, i, cm[i, j],\n","                 horizontalalignment=\"center\",\n","                 color=\"white\" if cm[i, j] > thresh else \"black\")\n","    plt.tight_layout()\n","    plt.ylabel('True label')\n","    plt.xlabel('Predicted label')\n","    plt.show()"],"execution_count":5,"outputs":[]},{"cell_type":"code","metadata":{"id":"JTopstYNWcOS"},"source":["data_path = \"/Data\" #the path of orginal images\n","new_path = \"/Pre_Data\" #the path of preprocced images\n","\n","width = height = 299\n","\n","if not os.path.exists(new_path):\n","  os.mkdir(new_path)\n","\n","\n","for direc in os.listdir(data_path):\n","  directory_path = os.path.join(data_path, direc)\n","\n","\n","  for image_name in os.listdir(directory_path):\n","    image = cv.imread(os.path.join(directory_path, image_name))\n","    \n","    print(f'Processing Image {image_name} start...')\n","\n","    #Crop Image\n","    croped_image = crop_image(image)\n","    print('    Crop Image complete...' , croped_image.shape)\n","\n","    #Bias Corection\n","    bias_coreected_image = bias_correction(croped_image)\n","    print('    Bias Correction complete...')\n","\n","    #Reshape Image\n","    reshaped_image = reshape_image(bias_coreected_image, size=(width, height))\n","    print(f'   Image Reshaped from {image.shape} to {reshaped_image.shape}')\n","\n","    #Save Image\n","    file_name = os.path.join(os.path.join(new_path, direc), image_name)\n","    reshaped_image *= (255.0/reshaped_image.max())\n","    cv.imwrite(file_name, reshaped_image.astype(np.uint8))\n","    print('    Saved Image')\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"opsRijpzWiab"},"source":["##Splite Data"]},{"cell_type":"code","metadata":{"id":"x22s2p3s2oJs"},"source":["# split the data by train/val/test\n","IMG_PATH = 'MRI_DATA/Pre_Data/'\n","data_folder = 'MRI_DATA/splited_data/'\n","for CLASS in os.listdir(IMG_PATH):\n","    if not CLASS.startswith('.'):\n","        IMG_NUM = len(os.listdir(IMG_PATH + CLASS))\n","        for (n, FILE_NAME) in enumerate(os.listdir(IMG_PATH + CLASS)):\n","            img = IMG_PATH + CLASS + '/' + FILE_NAME\n","            if n < 5:\n","                shutil.copy(img, data_folder + 'test/' + CLASS + '/' + FILE_NAME)\n","            elif n < 0.8*IMG_NUM:\n","                shutil.copy(img, data_folder + 'train/'+ CLASS + '/' + FILE_NAME)\n","            else:\n","                shutil.copy(img, data_folder + 'val/'+ CLASS + '/' + FILE_NAME)\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"pOELDOV81pi_"},"source":["# Data augmentation and normalization for training\n","# Just normalization for validation\n","data_transforms = {\n","    'train': transforms.Compose([\n","        transforms.RandomResizedCrop(224), #crop a random 244 by 24 image from the input image\n","        transforms.RandomHorizontalFlip(), #apply the flip algorithm randomly\n","        transforms.ToTensor(), #convert the image to a pytorch tensor\n","        transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5]) #normalize the each chanle of image\n","    ]),\n","    'val': transforms.Compose([\n","        transforms.Resize(256),\n","        transforms.CenterCrop(224),#crop a 244 by 24 image from the input image\n","        transforms.ToTensor(),#convert the image to a pytorch tensor\n","        transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])#normalize the each chanle of image\n","    ]),\n","    'test': transforms.Compose([\n","        transforms.Resize(256),\n","        transforms.CenterCrop(224),#crop a 244 by 24 image from the input image\n","        transforms.ToTensor(),#convert the image to a pytorch tensor\n","        transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])#normalize the each chanle of image\n","    ]),\n","\n","}\n","batch_size = {'train':8, 'val':8, 'test':1} #define batch_size for each set\n","data_dir = '/content/drive/My Drive/Lesion_Segmentation/MRI_DATA/splited_data/' #the path of images\n","image_datasets = {x: datasets.ImageFolder(os.path.join(data_dir, x),\n","                                          data_transforms[x])\n","                  for x in ['train', 'val', 'test']} # define a variable as a dataset for each dataset\n","dataloaders = {x: torch.utils.data.DataLoader(image_datasets[x], batch_size=batch_size[x],\n","                                             shuffle=True, num_workers=4)\n","              for x in ['train', 'val', 'test']} #define a dataloader for each dataset\n","dataset_sizes = {x: len(image_datasets[x]) for x in ['train', 'val', 'test']}\n","class_names = image_datasets['train'].classes\n","\n","device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"CZ3AYvSh1pjE"},"source":["def imshow(inp, title=None):\n","    \"\"\"Imshow for Tensor.\"\"\"\n","    inp = inp.numpy().transpose((1, 2, 0))\n","    mean = np.array([0.485, 0.456, 0.406])\n","    std = np.array([0.229, 0.224, 0.225])\n","    inp = std * inp + mean\n","    inp = np.clip(inp, 0, 1)\n","    plt.imshow(inp)\n","    if title is not None:\n","        plt.title(title)\n","    plt.pause(0.001)  # pause a bit so that plots are updated\n","\n","\n","# Get a batch of training data\n","inputs, classes = next(iter(dataloaders['train']))\n","\n","# Make a grid from batch\n","out = torchvision.utils.make_grid(inputs)\n","\n","imshow(out, title=[class_names[x] for x in classes])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"GLCZoL5U1pjN"},"source":["def train_model(model, criterion, optimizer, scheduler, num_epochs=25):\n","    since = time.time()\n","    acc_train=[]\n","    acc_val=[]\n","    loss_train=[]\n","    loss_val=[]\n","\n","    best_model_wts = copy.deepcopy(model.state_dict())\n","    best_acc = 0.0\n","\n","    for epoch in range(num_epochs):\n","        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n","        print('-' * 10)\n","\n","         # Each epoch has a training and validation phase\n","        for phase in ['train', 'val']:\n","            if phase == 'train':\n","                model.train() # Set model to train mode\n","            else:\n","                model.eval() # Set model to evaluate mode\n","\n","            running_loss = 0.0\n","            running_corrects = 0\n","\n","            for inputs, labels in dataloaders[phase]:\n","                inputs = inputs.to(device)\n","                labels = labels.to(device)\n","                 # zero the parameter gradients\n","                optimizer.zero_grad()\n","\n","                # forward\n","                # track history if only in train\n","                with torch.set_grad_enabled(phase == 'train'):\n","                    outputs = model(inputs)\n","                    _, preds = torch.max(outputs, 1)\n","                    loss = criterion(outputs, labels)\n","                    # backward + optimize only if in training phase\n","                    if phase == 'train':\n","                        loss.backward()\n","                        optimizer.step()\n","\n","                running_loss += loss.item() * inputs.size(0)\n","                running_corrects += torch.sum(preds == labels.data)\n","            if phase == 'train':\n","                scheduler.step()\n","\n","            epoch_loss = running_loss / dataset_sizes[phase]\n","            epoch_acc = running_corrects.double() / dataset_sizes[phase]\n","\n","            print('{} Loss: {:.4f} Acc: {:.4f}'.format(\n","                phase, epoch_loss, epoch_acc))\n","            \n","            if phase == 'val':\n","                acc_val.append(epoch_acc)\n","                loss_val.append(epoch_loss)\n","            else:\n","                acc_train.append(epoch_acc)\n","                loss_train.append(epoch_loss)\n","            \n","            # deep copy the model\n","            if phase == 'val' and epoch_acc > best_acc:\n","                best_acc = epoch_acc\n","                best_model_wts = copy.deepcopy(model.state_dict())\n","\n","        print()\n","\n","    time_elapsed = time.time() - since\n","    print('Training complete in {:.0f}m {:.0f}s'.format(\n","        time_elapsed // 60, time_elapsed % 60))\n","    print('Best val Acc: {:4f}'.format(best_acc))\n","\n","    # load best model weights\n","    model.load_state_dict(best_model_wts)\n","    return model, loss_train, loss_val, acc_train, acc_val\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"L_gGLVnz1pjR"},"source":["def visualize_model(model, num_images=6):\n","    was_training = model.training\n","    model.eval()\n","    images_so_far = 0\n","    fig = plt.figure()\n","\n","    with torch.no_grad():\n","        for i, (inputs, labels) in enumerate(dataloaders['val']):\n","            inputs = inputs.to(device)\n","            labels = labels.to(device)\n","\n","            outputs = model(inputs)\n","            _, preds = torch.max(outputs, 1)\n","\n","            for j in range(inputs.size()[0]):\n","                images_so_far += 1\n","                ax = plt.subplot(num_images//2, 2, images_so_far)\n","                ax.axis('off')\n","                ax.set_title('predicted: {}'.format(class_names[preds[j]]))\n","                imshow(inputs.cpu().data[j])\n","\n","                if images_so_far == num_images:\n","                    model.train(mode=was_training)\n","                    return\n","        model.train(mode=was_training)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"JA0n206K1pjY","colab":{"base_uri":"https://localhost:8080/","height":105,"referenced_widgets":["c0f1a4a7212744baab0ccfb3842ea1c9","9734328003f946029f18655055b389eb","15f0649e3ce2481796e452b1f06d337e","15e593a16d72486fbe41f1ebb392362a","35348b3d6b2149a8ae79680d88659c68","c2029bb8fccb401b99346e5eb3c6a5da","9a234e7d86084763bc89a56ec40d61b5","02146dd15cb84ad5ae9c346ee3b0f911"]},"executionInfo":{"status":"ok","timestamp":1598687593490,"user_tz":-270,"elapsed":12003,"user":{"displayName":"Mehdi Sadeghi","photoUrl":"","userId":"06637468399330131472"}},"outputId":"d051a085-7d34-4c34-d2ca-e0019f2e2ca8"},"source":["model_ft = models.resnet18(pretrained=True)\n","num_ftrs = model_ft.fc.in_features\n","\n","# Here the size of each output sample is set to 2.\n","# Alternatively, it can be generalized to nn.Linear(num_ftrs, len(class_names)).\n","model_ft.fc = nn.Linear(num_ftrs, 2)\n","\n","model_ft = model_ft.to(device)\n","\n","criterion = nn.CrossEntropyLoss()\n","\n","# Observe that all parameters are being optimized\n","optimizer_ft = optim.SGD(model_ft.parameters(), lr=0.001, momentum=0.9)\n","\n","# Decay LR by a factor of 0.1 every 7 epochs\n","exp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=7, gamma=0.1)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Downloading: \"https://download.pytorch.org/models/resnet18-5c106cde.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-5c106cde.pth\n"],"name":"stderr"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"c0f1a4a7212744baab0ccfb3842ea1c9","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, max=46827520.0), HTML(value='')))"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"7X3WOkXO1pjc"},"source":["model_ft, loss_train, loss_val, acc_train, acc_val = train_model(model_ft, criterion, optimizer_ft, exp_lr_scheduler,num_epochs=40)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"wtFmozPLtSwp"},"source":["plt.plot(loss_train)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"PMMXSe5A1pjg"},"source":["visualize_model(model_ft)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"FTeY5ygP1pjk"},"source":["model_conv = torchvision.models.resnet18(pretrained=True)\n","for param in model_conv.parameters():\n","    param.requires_grad = False\n","\n","# Parameters of newly constructed modules have requires_grad=True by default\n","num_ftrs = model_conv.fc.in_features\n","model_conv.fc = nn.Linear(num_ftrs, 2)\n","\n","model_conv = model_conv.to(device)\n","\n","criterion = nn.CrossEntropyLoss()\n","\n","# Observe that only parameters of final layer are being optimized as\n","# opposed to before.\n","optimizer_conv = optim.SGD(model_conv.fc.parameters(), lr=0.001, momentum=0.9)\n","\n","# Decay LR by a factor of 0.1 every 7 epochs\n","exp_lr_scheduler = lr_scheduler.StepLR(optimizer_conv, step_size=7, gamma=0.1)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"x6DN8mrL1pjq"},"source":["model_conv, loss_train, loss_val, acc_train, acc_val = train_model(model_conv, criterion, optimizer_conv,\n","                         exp_lr_scheduler, num_epochs=25)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"D2lkDwrW1pjw"},"source":["visualize_model(model_conv)\n","\n","plt.ioff()\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"g85SMEH98-jx"},"source":["plt.plot(acc_train)\n","plt.xlabel('Itration')\n","plt.ylabel('Accuracy(%)')\n","plt.title('Train Accuracy')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"BjBbt8i6s2j7"},"source":["plt.plot(acc_val)\n","plt.xlabel('Itration')\n","plt.ylabel('Accuracy(%)')\n","plt.title('Validation Accuracy')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"56OMbUbwHln6"},"source":["plt.plot(loss_train)\n","plt.xlabel('Itration')\n","plt.ylabel('Binary Cross-Entropy Loss(%)')\n","plt.title('Train Loss')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"wg83aH0Z9lkg"},"source":["plt.plot(loss_val)\n","plt.xlabel('Itration')\n","plt.ylabel('Binary Cross-Entropy Loss(%)')\n","plt.title('Validation Loss')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"98Z2bc1099Ng"},"source":["# test the model\n","predicted_label = []\n","label = []\n","for inputs, labels in dataloaders['test']:\n","    inputs = inputs.to(device)\n","    labels = labels.to(device)\n","  \n","    outputs = model_conv(inputs)\n","    _, temp = torch.max(outputs, 1)\n","    predicted_label.append(int(temp.to('cpu').numpy())+1)\n","    label.append(int(labels.to('cpu').numpy()))\n","\n","print(f'The labels are: {label}')\n","print(f'The predicted labels are: {predicted_label}')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"VLqepLP5afIO"},"source":["accuracy = accuracy_score(label, predicted_label)\n","print('Test Accuracy = %.2f' % accuracy)\n","\n","confusion_mtx = confusion_matrix(label, predicted_label) \n","cm = plot_confusion_matrix(confusion_mtx, classes = list(np.unique(label)), normalize=False, title='Test Confusion Matrix')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"NXNAvm3rsaoL"},"source":["transform = transforms.Compose([\n","        transforms.RandomResizedCrop(224), #crop a random 244 by 24 image from the input image\n","        transforms.RandomHorizontalFlip(), #apply the flip algorithm randomly\n","        transforms.ToTensor(), #convert the image to a pytorch tensor\n","        transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5]) #normalize the each chanle of image\n","    ])\n","\n","data_dir = '/content/drive/My Drive/Lesion_Segmentation/MRI_DATA/Pre_Data/' #the path of images\n","image_datasets = datasets.ImageFolder(data_dir, transform)\n","dataloaders = torch.utils.data.DataLoader(image_datasets, batch_size=1, shuffle=False, num_workers=4)\n","          \n","device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"luDAZa2wtTVm"},"source":["# test the model\n","predicted_label = []\n","label = []\n","for inputs, labels in dataloaders:\n","    inputs = inputs.to(device)\n","    labels = labels.to(device)\n","  \n","    outputs = model_conv(inputs)\n","    _, temp = torch.max(outputs, 1)\n","    predicted_label.append(int(temp.to('cpu').numpy()))\n","    label.append(int(labels.to('cpu').numpy()))\n","\n","print(f'The labels are: {label}')\n","print(f'The predicted labels are: {predicted_label}')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"qn7baUWfteCD"},"source":["accuracy = accuracy_score(label, predicted_label)\n","print('Overall Accuracy = %.2f' % accuracy)\n","\n","confusion_mtx = confusion_matrix(label, predicted_label) \n","cm = plot_confusion_matrix(confusion_mtx, classes = list(np.unique(label)), normalize=False, title='Overall Confusion Matrix')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"jZN-xFX-t9Ld","executionInfo":{"status":"ok","timestamp":1631352094288,"user_tz":-270,"elapsed":7,"user":{"displayName":"Mehdi Sadeghi","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"06637468399330131472"}}},"source":[""],"execution_count":null,"outputs":[]}]}